
# PGTransfer

[![Go Version](https://img.shields.io/badge/Go-1.24.4+-blue.svg)](https://golang.org)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-13+-blue.svg)](https://postgresql.org)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)

**PGTransfer** is a powerful, secure, and user-friendly command-line tool for PostgreSQL data operations. Built with Go, it provides seamless data import/export capabilities with advanced features like SSH tunneling, connection profiles, and interactive configuration.

## ‚ú® Features

- **üîÑ Data Transfer**: Import and export PostgreSQL tables to/from CSV format
- **üóÑÔ∏è Database Migration**: Complete database migration with schema, data, and selective table transfer
- **‚ö° Batch Processing**: High-performance batch operations with configurable batch sizes
- **üîê Secure Connections**: Support for SSL/TLS and SSH tunnel connections
- **üë§ Profile Management**: Reusable connection profiles with secure credential storage
- **üñ•Ô∏è Interactive Setup**: User-friendly interactive profile configuration
- **üìä Progress Tracking**: Real-time progress indicators for large data operations
- **üîÑ Rollback Support**: Automatic backup creation and rollback capabilities for migrations
- **üìù Comprehensive Logging**: Detailed JSON-formatted operation logs
- **üîë Multiple Authentication**: Support for password and SSH key authentication
- **üíæ Memory Efficient**: Optimized for large datasets with efficient memory usage
- **üéØ Smart Routing**: Automatic optimization for different data sizes

## üöÄ Quick Start

### Prerequisites

- **Go**: Version 1.24.4 or newer
- **PostgreSQL**: Version 13 or newer
- **SSH Client**: For SSH tunnel connections (optional)

### Installation

#### From Source

```bash
git clone https://github.com/andymarthin/pgtransfer.git
cd pgtransfer
go build -o pgtransfer
sudo mv pgtransfer /usr/local/bin/
```

#### Verify Installation

```bash
pgtransfer --help
```

## üìã Usage

### Profile Management

PGTransfer uses connection profiles to manage database credentials and settings securely.

#### Create a New Profile (Interactive)

```bash
pgtransfer profile add myprofile
```

The interactive setup will guide you through:
- Database connection details
- SSL configuration
- SSH tunnel settings (optional)
- Authentication method selection

#### Create a Profile (Command Line)

```bash
pgtransfer profile add production \
  --user myuser \
  --password mypassword \
  --host db.example.com \
  --port 5432 \
  --database myapp \
  --sslmode require
```

#### SSH Tunnel Configuration

For secure connections through a bastion host:

```bash
pgtransfer profile add staging \
  --user dbuser \
  --host localhost \
  --port 5432 \
  --database staging_db \
  --ssh-host bastion.example.com \
  --ssh-user ubuntu \
  --ssh-key ~/.ssh/id_rsa
```

#### List Profiles

```bash
pgtransfer profile list
```

#### Test Connection

```bash
pgtransfer test-connection myprofile
```

#### Update Existing Profile

When updating an existing profile, PGTransfer will show current values as defaults:

```bash
pgtransfer profile add myprofile
```

```
‚ö†Ô∏è  Profile 'myprofile' already exists with the following configuration:
  Database: myuser@db.example.com:5432/myapp
  SSL Mode: require
  SSH: Not configured

Do you want to overwrite it? [y/N]: y

üß© Interactive Profile Setup
Database user [myuser]: newuser
Database password: 
Database host [db.example.com]: 
Database port [5432]: 
Database name [myapp]: 
SSL mode [require]: 
Use SSH tunnel? (y/N) [n]: 
```

### Data Operations

#### Export Data

Export a table to CSV:

```bash
pgtransfer export csv myprofile public.users users_export.csv --headers
```

Export with schema flag (defaults to 'public' if not specified):

```bash
pgtransfer export csv myprofile users users_export.csv --headers --schema public
```

With custom batch size for large datasets:

```bash
pgtransfer export csv myprofile public.users large_export.csv --headers --batch-size 1000
```

#### Database Dump Export

Export complete database to SQL dump:

```bash
pgtransfer export dump myprofile database_backup.sql
```

Export with custom format and compression:

```bash
pgtransfer export dump myprofile backup.dump --format custom --compress
```

Export schema only (no data):

```bash
pgtransfer export dump myprofile schema_backup.sql --schema-only
```

Export data only (no schema):

```bash
pgtransfer export dump myprofile data_backup.sql --data-only
```

Export specific tables:

```bash
pgtransfer export dump myprofile users_backup.sql --table users --table orders
```

Export with advanced options:

```bash
pgtransfer export dump myprofile full_backup.dump \
  --format custom \
  --compress \
  --verbose \
  --timeout 300
```

**Supported Formats:**
- `plain` (default): Standard SQL text format
- `custom`: PostgreSQL custom binary format (supports compression)
- `directory`: Directory format for parallel processing
- `tar`: TAR archive format

#### Import Data

Import CSV data to a table:

```bash
pgtransfer import csv myprofile public.users users_import.csv --headers
```

Import with schema flag (defaults to 'public' if not specified):

```bash
pgtransfer import csv myprofile users users_import.csv --headers --schema public
```

Import with custom batch size and overwrite existing data:

```bash
pgtransfer import csv myprofile public.users users_import.csv --headers --batch-size 1000 --overwrite
```

Import from SQL dump file (automatically detects format):

```bash
pgtransfer import dump myprofile backup.sql
```

### Database Migration

PGTransfer provides comprehensive database migration capabilities for transferring entire databases or specific components between PostgreSQL instances. You can use either different profiles or the same profile with database overrides.

#### Migration Modes

**Mode 1: Different Profiles**
Use separate connection profiles for source and target:

```bash
pgtransfer migrate database source_profile target_profile
```

**Mode 2: Same Profile with Database Override**
Use the same connection profile but specify different database names:

```bash
pgtransfer migrate database profile --source-database source_db --target-database target_db
```

#### Full Database Migration

Migrate complete database (schema + data) with different profiles:

```bash
pgtransfer migrate database source_profile target_profile
```

Migrate complete database using same profile:

```bash
pgtransfer migrate database myprofile --source-database prod_db --target-database staging_db
```

#### Schema-Only Migration

Transfer only the database structure with different profiles:

```bash
pgtransfer migrate database source_profile target_profile --schema-only
```

Transfer only the database structure using same profile:

```bash
pgtransfer migrate database myprofile --source-database prod_db --target-database dev_db --schema-only
```

#### Data-Only Migration

Transfer only data (assumes target schema exists) with different profiles:

```bash
pgtransfer migrate database source_profile target_profile --data-only
```

Transfer only data using same profile:

```bash
pgtransfer migrate database myprofile --source-database source_db --target-database target_db --data-only
```

#### Selective Table Migration

Migrate specific tables with different profiles:

```bash
pgtransfer migrate database source_profile target_profile --tables users,orders,products
```

Migrate specific tables using same profile:

```bash
pgtransfer migrate database myprofile --source-database db1 --target-database db2 --tables users,orders,products
```

#### Migration with Validation

Pre-validate migration before execution with different profiles:

```bash
pgtransfer migrate database source_profile target_profile --validate --verbose
```

Pre-validate migration using same profile:

```bash
pgtransfer migrate database myprofile --source-database source_db --target-database target_db --validate --verbose
```

#### Migration with Rollback Support

Enable automatic backup for rollback capability with different profiles:

```bash
pgtransfer migrate database source_profile target_profile --enable-rollback
```

Enable automatic backup using same profile:

```bash
pgtransfer migrate database myprofile --source-database source_db --target-database target_db --enable-rollback
```

#### Advanced Migration Options

With different profiles:

```bash
pgtransfer migrate database source_profile target_profile \
  --tables users,orders \
  --validate \
  --enable-rollback \
  --timeout 1800 \
  --batch-size 1000 \
  --verbose
```

With same profile and database override:

```bash
pgtransfer migrate database myprofile \
  --source-database source_db \
  --target-database target_db \
  --tables users,orders \
  --validate \
  --enable-rollback \
  --timeout 1800 \
  --batch-size 1000 \
  --verbose
```

#### Rollback Operations

Rollback a migration using a backup file:

```bash
pgtransfer migrate rollback target_profile /path/to/backup.sql --verbose
```

#### Migration Features

- **üîç Pre-Migration Validation**: Connection testing and schema compatibility checks
- **üìä Progress Tracking**: Real-time progress indicators with elapsed time
- **üîÑ Rollback Support**: Automatic backup creation for safe rollbacks
- **‚öôÔ∏è Flexible Options**: Schema-only, data-only, or selective table migration
- **üõ°Ô∏è Error Handling**: Comprehensive error reporting and graceful failure handling
- **‚è±Ô∏è Timeout Control**: Configurable timeouts for long-running migrations
- **üìù Verbose Logging**: Detailed operation logs for troubleshooting

#### Migration Examples

**Production Database Refresh (Different Profiles):**
```bash
# Create backup and migrate with rollback support
pgtransfer migrate database production staging --enable-rollback --verbose

# If issues occur, rollback
pgtransfer migrate rollback staging /path/to/backup_20241028_092222.sql
```

**Production Database Refresh (Same Profile):**
```bash
# Create backup and migrate with rollback support using same profile
pgtransfer migrate database myprofile \
  --source-database prod_db \
  --target-database staging_db \
  --enable-rollback --verbose

# If issues occur, rollback
pgtransfer migrate rollback staging /path/to/backup_20241028_092222.sql
```

**Development Environment Setup (Different Profiles):**
```bash
# Schema-only migration for development
pgtransfer migrate database production dev --schema-only

# Add sample data separately
pgtransfer migrate database sample_data dev --data-only --tables users,products
```

**Development Environment Setup (Same Profile):**
```bash
# Schema-only migration for development using same profile
pgtransfer migrate database myprofile \
  --source-database production_db \
  --target-database dev_db \
  --schema-only

# Add sample data separately
pgtransfer migrate database myprofile \
  --source-database sample_data_db \
  --target-database dev_db \
  --data-only --tables users,products
```

**Selective Data Migration (Different Profiles):**
```bash
# Migrate specific tables with validation
pgtransfer migrate database prod_replica analytics \
  --tables user_events,transactions,metrics \
  --validate \
  --timeout 3600
```

**Selective Data Migration (Same Profile):**
```bash
# Migrate specific tables with validation using same profile
pgtransfer migrate database myprofile \
  --source-database prod_replica_db \
  --target-database analytics_db \
  --tables user_events,transactions,metrics \
  --validate \
  --timeout 3600
```

## ‚ö° Batch Processing & Performance

PGTransfer features advanced batch processing capabilities optimized for large datasets. The system automatically handles memory management and provides real-time progress tracking.

### Batch Size Optimization

The default batch size is **500 rows**, which provides optimal performance for most use cases. However, you can customize this based on your specific needs:

#### Performance Benchmarks (1M Records)

| Batch Size | Export Time | Import Time | Memory Usage | Recommended For |
|------------|-------------|-------------|--------------|-----------------|
| 100        | 2m 38s      | ~4m         | Low          | Memory-constrained environments |
| 500 (default) | 3.20s   | 3m 19s      | Moderate     | ‚úÖ **Optimal for most cases** |
| 1,000      | 3.84s       | ~3m         | Moderate     | Large datasets with good network |
| 5,000      | 6.84s       | ~2m 30s     | Higher       | High-performance environments |
| 10,000     | 5.25s       | ~2m         | High         | Maximum throughput scenarios |

### Large Dataset Examples

#### Export 1 Million Records

```bash
# Optimal performance with default batch size
pgtransfer export csv production public.large_table export_1m.csv --headers

# High-throughput export for fast networks
pgtransfer export csv production public.large_table export_1m.csv --headers --batch-size 5000

# Memory-efficient export for constrained environments
pgtransfer export csv production public.large_table export_1m.csv --headers --batch-size 100
```

#### Import Large CSV Files

```bash
# Standard import with progress tracking
pgtransfer import csv production public.target_table large_data.csv --headers

# High-speed import with larger batches
pgtransfer import csv production public.target_table large_data.csv --headers --batch-size 2000

# Safe import with table replacement
pgtransfer import csv production public.target_table large_data.csv --headers --overwrite
```

### Progress Tracking

All operations display real-time progress with:
- **Progress Bar**: Visual completion indicator
- **Speed Metrics**: Rows processed per second
- **Time Estimates**: Elapsed and estimated remaining time
- **Memory Usage**: Current system resource utilization

Example output:
```
Exporting public.users (elapsed 3s) 100% |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| (1000000/1000000, 322199 it/s)
‚úÖ Exported 1000000 rows to users_export.csv (batch size: 500)
üïí Duration: 3.15s
```

### Memory Management

PGTransfer implements intelligent memory management:
- **Streaming Processing**: Data is processed in chunks, not loaded entirely into memory
- **Connection Pooling**: Efficient database connection reuse
- **Garbage Collection**: Automatic cleanup of processed batches
- **Resource Monitoring**: Built-in memory usage tracking

## üìä CSV Data Type Handling

PGTransfer provides intelligent CSV formatting that properly handles PostgreSQL data types for seamless import/export operations.

### Supported Data Types

| PostgreSQL Type | CSV Format | Example |
|----------------|------------|---------|
| `DATE` | `YYYY-MM-DD` | `2008-07-06` |
| `TIMESTAMP` | `YYYY-MM-DD HH:MM:SS` | `2025-10-28 01:59:38` |
| `NUMERIC(p,s)` | Decimal notation | `63942.00` |
| `INTEGER` | Plain number | `457719` |
| `VARCHAR/TEXT` | Quoted strings | `user_0457719` |
| `BOOLEAN` | `true`/`false` | `true` |
| `BYTEA` | String representation | Converted to readable format |

### Key Improvements

#### ‚úÖ Date and Timestamp Formatting
- **Before**: `1960-01-02 00:00:00 +0000 +0000` (with timezone info)
- **After**: `1960-01-02` (clean date format)
- **Timestamp**: `2025-10-28 01:59:38` (without timezone)

#### ‚úÖ Decimal and Numeric Handling
- **Before**: `[49 51 57 52 50 46 48 48]` (byte array representation)
- **After**: `63942.00` (proper decimal format)

#### ‚úÖ Null Value Handling
- **Consistent**: Empty strings for NULL values across all data types
- **Import Compatible**: Properly recognized during CSV import operations

### CSV Export Examples

#### Standard Table Export
```bash
# Export with proper formatting
pgtransfer export csv myprofile public.users users.csv --headers
```

#### Custom Query Export
```bash
# Export specific columns with formatting
pgtransfer export csv myprofile results.csv \
  --query "SELECT id, name, date_of_birth, salary FROM users WHERE active = true" \
  --headers
```

### CSV Import Compatibility

The improved formatting ensures seamless round-trip operations:

```bash
# Export data
pgtransfer export csv source_profile public.users exported_data.csv --headers

# Import to different database
pgtransfer import csv target_profile public.users_copy exported_data.csv --headers
```

**Result**: All data types are preserved correctly without manual formatting adjustments.

### Advanced Usage

#### Override Profile Database

```bash
pgtransfer export --profile myprofile --database different_db --table users --file export.csv
```

#### Skip Connection Testing

```bash
pgtransfer profile add myprofile --skip-test
```

#### Force Profile Overwrite

```bash
pgtransfer profile add myprofile --force
```

## üîß Configuration

### Configuration File

Profiles are stored in `~/.pgtransfer/config.yaml`:

```yaml
profiles:
  local:
    name: local
    user: postgres
    host: localhost
    port: 5432
    database: myapp_dev
    sslmode: disable
    ssh:
      enabled: false
  
  production:
    name: production
    user: app_user
    host: localhost
    port: 5432
    database: myapp_prod
    sslmode: require
    ssh:
      enabled: true
      host: bastion.example.com
      user: ubuntu
      port: 22
      key_path: /home/user/.ssh/id_rsa
      timeout: 10
```

### Logging

Logs are automatically saved to `~/.pgtransfer/logs/` in JSON format:

```json
{
  "timestamp": "2024-01-15T10:30:45Z",
  "level": "info",
  "operation": "export",
  "profile": "production",
  "table": "users",
  "file": "users_export.csv",
  "status": "success",
  "duration_ms": 1250,
  "rows_processed": 10000
}
```

## üê≥ Development & Testing

### Local Development with Docker

Create a `docker-compose.yml` for testing:

```yaml
version: "3.8"
services:
  postgres:
    image: postgres:15
    container_name: pgtransfer_test
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: testdb
    ports:
      - "5432:5432"
    volumes:
      - ./test_data:/docker-entrypoint-initdb.d
```

Start the test environment:

```bash
docker-compose up -d
```

### Sample Test Data

Create `test_data/init.sql`:

```sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    username VARCHAR(50) NOT NULL,
    email VARCHAR(100) UNIQUE NOT NULL,
    age INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

INSERT INTO users (username, email, age) VALUES
    ('alice', 'alice@example.com', 30),
    ('bob', 'bob@example.com', 25),
    ('charlie', 'charlie@example.com', 35),
    ('diana', 'diana@example.com', 28);
```

### Running Tests

```bash
go test ./...
```

## üîí Security Features

- **Secure Password Input**: Passwords are never echoed to the terminal
- **SSH Key Support**: Supports both password and key-based SSH authentication
- **SSL/TLS Encryption**: Configurable SSL modes for database connections
- **Credential Storage**: Secure local storage of connection profiles
- **SSH Tunnel Encryption**: All data transfers through SSH tunnels are encrypted

## ü§ù Contributing

We welcome contributions! Please see our [Contributing Guidelines](CONTRIBUTING.md) for details.

### Development Setup

1. Fork the repository
2. Create a feature branch: `git checkout -b feature/amazing-feature`
3. Make your changes and add tests
4. Commit your changes: `git commit -m 'Add amazing feature'`
5. Push to the branch: `git push origin feature/amazing-feature`
6. Open a Pull Request

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üÜò Support

- **Issues**: [GitHub Issues](https://github.com/andymarthin/pgtransfer/issues)
- **Discussions**: [GitHub Discussions](https://github.com/andymarthin/pgtransfer/discussions)
- **Documentation**: [Wiki](https://github.com/andymarthin/pgtransfer/wiki)

## üèÜ Acknowledgments

- Built with [Go](https://golang.org/)
- PostgreSQL driver: [pq](https://github.com/lib/pq)
- SSH client: [golang.org/x/crypto/ssh](https://pkg.go.dev/golang.org/x/crypto/ssh)
- CLI framework: [Cobra](https://github.com/spf13/cobra)

---

**Made with ‚ù§Ô∏è by the PGTransfer team**
